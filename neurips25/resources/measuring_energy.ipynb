{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-on Session: Benchmarking the Energy Consumption of Image Generation\n",
        "\n",
        "---\n",
        "\n",
        "**Make sure you switch the notebook runtime to GPU!**\n",
        "\n",
        "The tutorial will work with the default GPU runtime with NVIDIA T4.\n",
        "\n",
        "---\n",
        "\n",
        "We'll be using Hugging Face Diffusers to run a small SD Turbo diffusion model for text-to-image."
      ],
      "metadata": {
        "id": "1ueKoiBRDGsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"[%(asctime)s] [%(name)s:%(lineno)d] %(message)s\")\n",
        "logging.getLogger(\"zeus\").setLevel(logging.INFO)\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import AutoPipelineForText2Image\n",
        "\n",
        "pipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sd-turbo\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "eN1Tdy5jrAbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Zeus\n",
        "\n",
        "- GitHub: https://github.com/ml-energy/zeus\n",
        "- Documentation: https://ml.energy/zeus"
      ],
      "metadata": {
        "id": "87AQtKA4DfOC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKptJ2UUFL_Z"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install zeus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Measurement\n",
        "\n",
        "Zeus provides a few convenient classes for measurement.\n",
        "\n",
        "### `ZeusMonitor`\n",
        "\n",
        "The main `ZeusMonitor` class lets you measure the energy consumption of arbitrary *ranges* or *windows* of code. You let the monitor know the range of code by beginning and ending a **measurement window** under a consistent name.\n",
        "\n",
        "```python\n",
        "monitor.begin_window(\"much compute\")\n",
        "# Much\n",
        "# Compute\n",
        "measurement = monitor.end_window(\"much compute\")\n",
        "```\n",
        "\n",
        "These windows can be **nested** arbitrarily, and With `str`-based window names, you can just pass around the `ZeusMonitor` object everywhere and measure any range of execution as needed.\n",
        "\n",
        "### `PowerMonitor`\n",
        "\n",
        "This is another convenience class that measures the power draw of the GPU over time in the background. Just instantiating the class will automatically start power monitoring, and you can later query power draw over any range of time.\n",
        "\n",
        "```python\n",
        "# GPU index -> List of (timestamp, power)\n",
        "timeline: dict[int, list[tuple[float, float]]] = power_monitor.get_power_timeline(\"device_instant\")\n",
        "```"
      ],
      "metadata": {
        "id": "9TMDuA97Dk_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zeus.monitor import ZeusMonitor, PowerMonitor\n",
        "\n",
        "# Collects device power consumption in the background\n",
        "power_monitor = PowerMonitor()\n",
        "\n",
        "# Measures time and energy within \"windows\" of execution\n",
        "monitor = ZeusMonitor()"
      ],
      "metadata": {
        "id": "PEjuBunOGb71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Measuring Energy Consumption\n",
        "\n",
        "Our goal is to measure the **energy consumption of generating one image**.\n",
        "\n",
        "An important parameter that affects this is inference *batch size*: the number of images generated at the same time. So we want to normalize/average correctly."
      ],
      "metadata": {
        "id": "kNID0J57GZm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"A cinematic scene of Zeus throwing lightning\"\n",
        "num_repeats = 5\n",
        "batch_sizes = [1, 2, 4, 8]\n",
        "images = []\n",
        "latency_measurements = []\n",
        "energy_measurements = []\n",
        "start_time = time.time()\n",
        "monitor.reset_windows()\n",
        "\n",
        "monitor.begin_window(\"whole benchmark\")\n",
        "\n",
        "# Warm up\n",
        "for _ in range(5):\n",
        "    _ = pipe(prompt=[prompt] * 4, num_inference_steps=1, guidance_scale=0.0)\n",
        "\n",
        "# Measurement\n",
        "for batch_size in [1, 2, 4, 8]:\n",
        "\n",
        "    monitor.begin_window(\"image generation\")\n",
        "\n",
        "    # Run `num_repeats` repetitions for stable measurement\n",
        "    for _ in range(num_repeats):\n",
        "        output = pipe(prompt=[prompt] * batch_size, num_inference_steps=4, guidance_scale=0.0)\n",
        "        images.extend(output.images)\n",
        "\n",
        "    ###################### FIXME #######################\n",
        "    # The monitor has to know from where to where we're\n",
        "    # trying to measure the energy of.\n",
        "    measurement = monitor.end_window(\"totally correct window name\")\n",
        "    ####################################################\n",
        "\n",
        "    latency = measurement.time / num_repeats\n",
        "\n",
        "    ################################ FIXME ###############################\n",
        "    # We want to know how much energy was consumed per generated image.\n",
        "    energy_per_image = measurement.total_energy\n",
        "    ######################################################################\n",
        "\n",
        "    latency_measurements.append(latency)\n",
        "    energy_measurements.append(energy_per_image)\n",
        "    print(f\"batch size: {batch_size}, generation time: {latency} s, energy per generation: {energy_per_image:.2f} J\")\n",
        "\n",
        "measurement = monitor.end_window(\"whole benchmark\")\n",
        "print(f\"The whole benchmark consumed an aggregate of {measurement.total_energy} Joules.\")\n",
        "power_timeline = power_monitor.get_all_power_timelines(start_time=start_time - 5.0)\n",
        "\n",
        "# Draw images\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(5, 5), tight_layout=True)\n",
        "axes = axes.flatten()\n",
        "for ax, img in zip(axes, images):\n",
        "    ax.imshow(img)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# Validation\n",
        "if energy_measurements[0] < energy_measurements[-1]:\n",
        "    print(\"\\nOops, wanna double check the second FIXME?\\nThe monitor measures the energy consumption of `batch_size` many image generations.\\n\")\n",
        "    raise AssertionError\n",
        "elif not all(energy < 100.0 for energy in energy_measurements):\n",
        "    print(\"\\nOops, wanna double check the second FIXME?\\nWe measured running the same image generation batch `num_repeats` times.\\n\")\n",
        "    raise AssertionError\n",
        "else:\n",
        "    print(\"\\nLooks good! ðŸŽ‰\\n\")"
      ],
      "metadata": {
        "id": "NNNcsLFEKA7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Measurements\n",
        "\n",
        "Let's look at how our measurements look like. First run the cell below.\n",
        "\n",
        "### Plot 1: Changing Batch Size\n",
        "\n",
        "How does changing the batch size impact time and energy?\n",
        "\n",
        "**Interpretations:**\n",
        "- With a larger batch size, latency increases because the amount of computation increases.\n",
        "- Energy consumption **per image** decreases because the GPUâ€™s fixed costs (e.g., idle/static power, scheduling overhead) are amortized better over more parallel image generations.\n",
        "\n",
        "### Plot 2: Power Over Time\n",
        "\n",
        "We can visualize the `PowerMonitor`â€™s power draw timeline to better understand how GPU power usage behaves during image generation. Power readings tend to fluctuate significantly over time, especially for instantaneous measurements. Overall, the power draw pattern reflects how actively the GPU is being utilized during computation.  \n",
        "\n",
        "**Interpretations:**  \n",
        "- During image generation, the power draw stays close to **70 W**, which matches the **Thermal Design Power (TDP)** of the NVIDIA T4 GPU provided by Colab.\n",
        "- Even with a batch size of 1, the GPU is already quite well utilized (though not fully saturated) because T4 is a small GPU, and thus exhibits quite high power draw.  \n",
        "- Instant power draw can **exceeds 70 W** for short bursts. This is normal, since **TDP does not mean a hard cap** but a specification indicating that *the windowed average power draw* will not exceed 70 W."
      ],
      "metadata": {
        "id": "RgGdcpnrKWwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "fig, ax1 = plt.subplots(figsize=(5, 4), tight_layout=True)\n",
        "ax2 = ax1.twinx()\n",
        "ax1.plot(batch_sizes, energy_measurements, marker=\"o\")\n",
        "ax2.plot(batch_sizes, latency_measurements, marker=\"o\", color=\"black\")\n",
        "ax1.set_xlabel(\"Batch size\")\n",
        "ax1.set_ylabel(\"Energy per generation (J)\", color=\"C0\")\n",
        "ax2.set_ylabel(\"Latency (s)\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 4), tight_layout=True)\n",
        "ax.axhline(70, color=\"red\")\n",
        "timeline = power_timeline[\"device_instant\"][0]\n",
        "ax.plot([entry[0] for entry in timeline], [entry[1] for entry in timeline])\n",
        "ax.set_ylim(0)\n",
        "ax.set_xlabel(\"Time (s)\")\n",
        "ax.set_ylabel(\"Device instant power (W)\")"
      ],
      "metadata": {
        "id": "scy58lLTuqbG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}